<!DOCTYPE html>
<html>

<head>
	<meta name="generator" content="Hugo 0.62.0" />
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Sandeep Silwal </title><link rel="icon" type="image/png" href=infinity.png /><meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="alternate" type="application/rss+xml" href="https://sandeepsilwal.com/index.xml" title="Sandeep Silwal" />
	<meta property="og:title" content="Sandeep Silwal" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://sandeepsilwal.com/" />
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://sandeepsilwal.com/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://sandeepsilwal.com/css/main.css" /><link rel="stylesheet" type="text/css" href="https://sandeepsilwal.com/css/dark.css" media="(prefers-color-scheme: dark)" />

	
	<script src="https://sandeepsilwal.com/js/main.js"></script>

</head>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="ready()"></script>



<body>

	<div class="container wrapper">
		<div class="header">
	<h1 class="site-title"><a href="https://sandeepsilwal.com/">Sandeep Silwal</a> </h1>

<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="https://sandeepsilwal.com/publications">Publications</a>
			</li>
		
			
			<li>
				<a href="https://sandeepsilwal.com/teaching">Teaching</a>
			</li>
			
		</ul>
	</nav>
</div>


		
	<div class="info"> 
		<img src="https://sandeepsilwal.com/sandeep_photo_csail.jpg"> 
		<h4>Hello! I am an Assistant Professor at the Department of Computer Sciences at the University of Wisconsin-Madison. </h4>

		<h4> Previously, I completed my Ph.D at MIT under the watchful eyes of <a href="https://people.csail.mit.edu/indyk/">Piotr Indyk </a>. </h4>
		
		<h4> My interests are broadly in fast algorithm design. Recently, I've been working in the intersection of machine learning and classical algorithms by designing provable algorithms in various ML settings, such as efficient algorithms for processing large datasets, as well as using ML to inspire and motivate algorithm design. </h4>

		<h4> Here are some high-level questions that I have been thinking about: </h4>

		<ul style="font-weight: 100; font-size: 17px; font-style: italic;">
		<li> Can we extend current search and retreival machinery to tackle new emerging challenges?  </li>
		<li> Can we learn "good algorithms" with provable guarantees directly from data? </li>
		<li> How can principes of sublinear algorithm design help tackle bottlenecks in big data computing? </li>
		<li> To what extent are sublinear algorithms and differential privacy related? </li>
		</ul>





		<h4>Feel free to contact me at {my last name}@cs.wisc.edu. </h4>

		<h4> <b> Prospective students:</b> I am looking to hire graduate students who are broadly interested in the interplay between algorithms and machine learning. If the above directions or the papers below seem interesting to you, feel free to reach out to me. </h4>

	</div>




	<div class="papers">  

	<a href="javascript:story('Selected')"><h2 style="padding-bottom:10px"> > Selected Publications </h2></a>

	<div id="Selected" style="display: none;">

		<ul>

			<li>  
	  			<p> <b> A Bi-metric Framework for Fast Similarity Search  </b>
	  				<br> Haike Xu*, Sandeep Silwal, Piotr Indyk
	  				<br> <em> Preprint</em>  [<a href="https://arxiv.org/abs/2406.02891">pdf</a>] [<a href="javascript:story('XSI24')">story</a>] [<a href="https://www.youtube.com/watch?v=W1ww23ZJC3s">video</a>] </p>

		  			<div id="XSI24" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p>  We propose a new "bi-metric" framework for designing nearest neighbor data structures. Our framework assumes two dissimilarity functions: a *ground-truth* metric that is accurate but expensive to compute (e.g., embeddings from a large or expensive neural network to compare two sentences), and a *proxy* metric that is cheaper but less accurate (e.g., embeddings from a very small or local network). In both theory and practice, we show how to construct data structures using only the proxy metric such that the query procedure achieves the accuracy of the expensive metric, while only using a limited number of calls to both metrics. In many cases, we can beat the popular "re-ranking" baseline through the use of some pretty cool graph-based data-structures!</p></div>
	  		</li>



			<li>  
	  			<p> <b>Efficiently Computing Similarities to Private Datasets </b>
	  				<br> Arturs Backurs, Zinan Lin, Sepideh Mahabadi, Sandeep Silwal, Jakub Tarnawski
	  				<br> <em> ICLR 2024</em>  [<a href="https://arxiv.org/abs/2403.08917">pdf</a>] [<a href="javascript:story('BLMST23')">story</a>] [<a href="https://www.youtube.com/watch?v=WI4B9ep5is0">video</a>] </p>


	  			<div id="BLMST23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> Imagine a hospital which has a (private) dataset of patients with a particular ailment. We would like to measure the similarity between this dataset and any new patient in hopes of diagnosing them. The paper abstracts this natural problem as follows: given a datset $X \subset \mathbb{R}^d$, output a private datastructure $D$. On any query vector $y$, $D(y)$ computes $\sum_{x \in X} f(x,y)$ where $f$ is a desired similarity function (e.g. distance function or kernel function). We want $D$ itself to be private so it can handle an arbitrary number of queries (for ex, image if $D$ always outputs $42$. This is private but not very accurate so we want to do better). The paper explores several natural choices of $f$ and shows how to exploit the "geometry" of $f$ to create $D$ with low error.</p></div>

	  		</li>



			<li>  
	  			<p> <b>Improved Frequency Estimation Algorithms with and without Predictions </b>
	  				<br> Anders Aamand, Justin Y. Chen, Huy Nguyen, Sandeep Silwal, Ali Vakilian 
	  				<br> <em> <b>Spotlight Presentation</b> at NeurIPS 2023</em>  [<a href="https://arxiv.org/abs/2312.07535">pdf</a>] [<a href="javascript:story('ACNSV23')">story</a>] [<a href="https://https://www.youtube.com/watch?v=6nyH32X_p-s">video</a>] </p>


	  			<div id="ACNSV23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We consider the fundamental problem of estimating frequencies of items arriving in a stream, given access to an oracle which predicts "heavy-hitters". Prior work gave theoretical guarantees for this setting under the further (natural) assumption that the underlying frequencies follow a Zipfian pattern. We show that under the Zipfian assumption, a much better frequency estimation algorithm already exists without the use of any predictions. Additionally incorporating predictions further improves its performance, as shown in our experiments. </p></div>

	  		</li>


	  		<li>  
	  			<p> <b> A Near-Linear Time Algorithm for the Chamfer Distance </b>
	  				<br> Ainesh Bakshi, Piotr Indyk, Rajesh Jayaram, Sandeep Silwal, Erik Waingarten
	  				<br> <em> NeurIPS 2023</em>  [<a href="https://arxiv.org/abs/2307.03043">pdf</a>] [<a href="javascript:story('BIJSW23')">story</a>] [<a href="https://www.youtube.com/watch?v=yhrRs6P3KKk">video</a>]</p>

	  			<div id="BIJSW23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> A popular measure of similarity between two large scale high-dimensional datasets is the optimal transport (or Earth Movers) distance. Unfortunately, this is a computationally expensive measure to compute. In this paper, we consider fast algorithms for computing the Chamfer distance, a relaxation of optimal transport. It is defined as the sum of nearest neighbor distances from every point in one dataset to the other. We give a linear time algorithm for computing this value and show that interestingly, outputting the underlying mapping (as opposed to just the value) between points that defines the Chamfer distance (conditionally) requires quadratic time.</p></div>
	  		</li>


			<li>  
	  			<p> <b>Data Structures for Density Estimation </b>
	  				<br> Anders Aamand, Alexandr Andoni, Justin Y Chen, Piotr Indyk, Shyam Narayanan, Sandeep Silwal
	  				<br> <em> ICML 2023</em>  [<a href="https://arxiv.org/abs/2306.11312">pdf</a>]  [<a href="javascript:story('AACINS23')">story</a>] [<a href="https://www.dropbox.com/scl/fi/kzc3gjt790786y8ouyrob/density_icml.pdf?rlkey=v0w3ml8xkug70d58xnpdyp958&dl=0">slides</a>] [<a href="https://slideslive.com/39003057/data-structures-for-density-estimation">video</a>]</p>

				<div id="AACINS23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We consider the following problem: we are given knowledge of $k$ discrete distributions $v_i$ for $1 \le i \le k$ over the domain $[n] = \{1, \cdots ,n\}$ which we can preprocess. Then we get samples from an unknown discrete distribution $p$, also over $[n]$. The goal is to output the closest distribution to $p$ among the $v_i$â€™s in TV distance (up to some small additive error). The state of the art algorithms require $\Theta(\log k)$ samples and run in near linear time.

				We introduce a fresh perspective on the problem and ask if we can output the closest distribution in sublinear time. This question is particularly motivated as the problem is a generalization of the traditional nearest neighbor search problem: if we take enough samples, we can learn $p$ explicitly up to low TV distance and then find the closest $v_i$ in $o(k)$ time using standard nearest neighbor search algorithms. However, this approach requires $\Omega(n)$ samples. Thus, it is natural to ask: can we obtain both sublinear number of samples and sublinear query time? We present some nice progress towards this question. </p></div>

	  		</li>



			<li>  
	  			<p> <b>KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals </b>
	  				<br> Sandeep Silwal*, Sara Ahmadian, Andrew Nystrom, Andrew McCallum, Deepak Ramachandran, Seyed Mehran Kazemi
	  				<br> <em> ICLR 2023 </em> [<a href="https://openreview.net/pdf?id=p0JSSa1AuV">pdf</a>] [<a href="javascript:story('SANMRK23')">story</a>] [<a href="https://www.dropbox.com/scl/fi/lohyojll541ovvfxccydl/Kwikbucks-ICLR-Pres.pdf?rlkey=xg60r5v33z4uiniyqoj8uw42l&st=451gfb6z&dl=0">slides</a>] </p>

	  				<div id="SANMRK23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> The work explores if we can efficiently query large ML models, more precisely large language models, to solve concrete downstream tasks. We focus on clustering: we are given a set of $n$ sentences of text documents and query access to a large LLM which provides high-quality pairwise similarity scores for every pair of sentences of texts. Ideally, we would query the model on all $\Theta(n^2)$ pairs to obtain a high-quality similarity matrix, which can be used for many types of clustering algorithms downstream. However, this is not feasible in practice due to the prohibitive resource costs required to make a large number of queries. For ex, even if $n = 10^6$, finding the 'ground truth' similarities as computed by the model requires on the order of $10^{12}$ queries! We solve this challenging problem via an algorithmic lens. First we narrow down to the concrete clustering problem of correlation clustering. Then, we use the fact that often times, very efficient but lower quality hints/predictions are available for text similarity. Using the idiosyncrasies of correlation clustering coupled with readily available hints, we are able to query the model a very limited number of times while still obtaining a clustering comparable to the hypothetical case where querying all pairs is feasible. We also theoretically model the setting and show that for correlation clustering, a linear number of queries is sufficient to obtain close to the optimal clustering quality. </p></div>
	  		</li>



 		   	<li>
	  			<p> <b>Faster Linear Algebra for Distance Matrices</b>  
	  			<br> Piotr Indyk, Sandeep Silwal

	  			<br> <b>Oral Presentation</b>  <em> at NeurIPS 2022 </em> [<a href="https://arxiv.org/abs/2210.15114">pdf</a>] [<a href="javascript:story('IS22')">story</a>] [<a href="https://www.dropbox.com/s/po80fnnbfpkzeyq/Faster%20Linear%20Algebra%20for%20Distance%20Matrices.pdf?dl=0">slides</a>] [<a href="https://www.youtube.com/watch?v=kjfxfcZGs4o">video</a>] </p>

				<div id="IS22" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> A distance matrix is defined as the matrix $A$ which records pairwise distances between $n$ data points in $\mathbb{R}^d$. They are ubiquitous in ML and data science as they capture similarity matrices and have applications in non-linear dimensionality reduction, visualization, clustering, etc. However, they require $\Omega(n^2)$ time and space for a dataset with $n$ points, making them quite difficult to compute on. We fill this gap and explore faster and scalable algorithms for distance matrices. There are three gems in the paper. (1) For the $\ell_1$ distance metric, we can compute $Ay$ for any vector $y$ in $O(nd)$ time, which is sublinear in the matrix size! Many foundational linear algebra algorithms (power method, iterative methods etc.) use matrix-vector products as the fundamental unit so $Ay$ the 'right' algorithmic primitive to consider. (2) The natural follow up question to (1) is if all metrics admit such fast matrix-vector speedups. This is <em> not </em> the case! We show that for the $\ell_{\infty}$ metric, compute $Ay$ for general $y$ requires $\Omega(n^2)$ time, assuming standard hardness assumptions. (3) Lastly, we consider the question of computing an approximate distance matrix, for example for the $\ell_2$ metric. The most natural way to do this, if one is familiar with some algorithmic tools, is to use the Johnson-Lindenstrauss (JL) lemma: simply project your points onto $O(\log n)$ dimensions and compute the distance matrix in the projected space, which requies $\Theta(n^2 \log n)$ time. Computing distances is in fact one of the most common useage of the JL lemma. We show that one can actually <em> go beyond </em> the JL lemma and comptue an approximate distance matrix in $O(n^2 \text{poly}(\log \log n))$ time! This is the result one would get if we could project onto $O(\text{poly}(\log \log n))$ dimensions using the JL lemma, but this is false as the JL lemma is tight! We use some nice metric compression results to obtian this result.</p></div>
	  		</li>


			<li>  
	  			<p> <b>Faster Fundamental Graph Algorithms via Learned Predictions </b>
	  				<br> Justin Y. Chen, Sandeep Silwal, Ali Vakilian, Fred Zhang
	  				<br> <em> ICML 2022 </em> [<a href="https://arxiv.org/abs/2204.12055">pdf</a>] [<a href="javascript:story('CSVZ22')">story</a>] [<a href="https://slideslive.com/38984142/faster-fundamental-graph-algorithms-via-learned-predictions">video</a>] </p>

	  				<div id="CSVZ22" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We extend the work of Dinitz et al. (NeurIPS, 2021) and study graph algorithms with the help of learned LP variables. We obtain a better bound for bipartite matching by taking the Hungarian method inspired algorithm of Dinitz et al. and incorporating a a flow perspective. We also give a learned variant of the classic Goldberg's algorithm for shortest-paths with negative edges. Interestingly, the vertex potentials that are often used in shortest-path algorithms are also the variables of an appropriate LP. Lastly, we obtain a host of other learning-based results by utilizing standard (efficient) reductions between graph problems. An interesting development was that for the matching case, the challenge was to utilize a (feasible) set of dual predictions whereas for shortest-paths, the challenge was to round a given set of predictions to first be feasible. After that, optimizing is easy: just run Dijkstra's! Our experiments were also on an interesting dataset: graphs where nodes represent countries and edges are currency exchange rates. By taking the logs of the rates, finding the most efficient way to transfer one currency to another is a cool textbook example of shortest-paths in action. </p></div>

	  		</li>



			<li>
	  			<p> <b>Randomized Dimensionality Reduction for Facility Location and Single-Linkage Clustering</b> 
	  			<br> Shyam Narayanan*, Sandeep Silwal*, Piotr Indyk, Or Zamir
	  			<br> <em> ICML 2021 </em> [<a href="https://arxiv.org/pdf/2107.01804.pdf">pdf</a>] [<a href="javascript:story('NSIZ21')">story</a>] [<a href="javascript:story('NSIZ21_video')">video</a>] </p>

				<div id="NSIZ21" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We obtain dimensionality reduction bounds for facility location clustering and the minimum spanning tree problem which go beyond the classical Johnson-Lindenstrauss (JL) lemma. For context, the work of Makarychev et al. (STOC 2019) showed that for $k$-means clustering, one can obtain a dimension bound in terms of $\log k$, rather than $\log n$ where $n$ is the number of points. In the problems we consider, there is no parameter "$k$" to parameterize the dimension bound. Instead we use the doubling dimension, which is a measure of intrinsic dimensionality of the dataset: it is at most $O(\log n)$ in the worst-case (i.e. JL bound) but can also be $O(1)$ regardless of $n$. To obtain our bounds, we exploit 'local' properties of our problems inspired by existing algorithms. The paper has a cool experiment and figure showing that doubling dimension is not just an artifact of analysis and its impact can actually be measured in practice. </p></div>


				<div id="NSIZ21_video" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> <a href="https://www.youtube.com/watch?v=H33OlvlqRTo">40 min video</a>  <br> <a href="https://icml.cc/virtual/2021/poster/9975">5 min video</a>  </p></div>

	  		</li>


			<li>
	  			<p> <b>Learning-based Support Estimation in Sublinear Time</b> 
	  			<br> Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, Tal Wagner

	  			<br> <em> <b>Spotlight Presentation</b> at ICLR 2021 </em> [<a href="https://arxiv.org/abs/2106.08396">pdf</a>] [<a href="javascript:story('EINRSW21')">story</a>] [<a href="javascript:story('EINRSW21_video')">video</a>] [<a href="https://www.dropbox.com/s/pcef3coyd6oh9jm/support%20poster%20iclr%2021.pdf?dl=0">poster</a>] </p>

				<div id="EINRSW21" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We consider the sample complexity of estimating the support size of a discrete distribution drawn from a subset of a possibly large domain $[n]$. Without any advice, the sample complexity is $O(n/\log n)$, barely sublinear. In our model, we assume approximate probabilities of each sample as advice. With this augmentation, the sample complexity drops significantly to $O(n^c)$ for $c &lt; 1$. Our experiments were based on internet search queries for which we could learn approximate underlying probabilities from historical data.</p></div>

				<div id="EINRSW21_video" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> <a href="https://harvard.zoom.us/rec/play/9hC2pgMhN_a-tQ3hzxJ6mBwsErS9ecjmwucPGSFjJrJ18NE4zDv5m4aP1AU9QAIOFV_GU5y4jO4a5S0.Azqt-cHOzyVNiox9?continueMode=true&_x_zm_rtaid=OvAuHrM3TySIM5t7Dvmv_w.1653660390076.17fb8b7ed468b3a9226c3a934ad0be8f&_x_zm_rhtaid=900">45 min video</a>  <br> <a href="https://iclr.cc/virtual/2021/poster/2622">10 min video</a>  </p></div>

	  		</li>


		</ul>


	<h5>*Authors are always listed in alphabetical order, except if authors are starred, in which case authors are listed in contribution order and starred authors have (equal) first-author contribution.</h5>

	</div>

	</div>



	


	<div class="footer wrapper">
	<nav class="nav">
		<div> <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a></div>
	</nav>
	</div>


</body>
</html>