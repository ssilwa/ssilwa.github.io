<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Publications - Sandeep Silwal</title><link rel="icon" type="image/png" href=infinity.png /><meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="alternate" type="application/rss+xml" href="https://sandeepsilwal.com/publications.xml" title="Sandeep Silwal" />
	<meta property="og:title" content="Publications" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://sandeepsilwal.com/publications" />
<meta property="og:updated_time" content="2019-12-30T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Publications"/>
<meta name="twitter:description" content=""/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://sandeepsilwal.com/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://sandeepsilwal.com/css/main.css" /><link rel="stylesheet" type="text/css" href="https://sandeepsilwal.com/css/dark.css" media="(prefers-color-scheme: dark)" />

	<script src="https://sandeepsilwal.com/js/main.js"></script>

</head>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="ready()"></script>


<body>
	<div class="container wrapper tags">
		<div class="header">
	<img style="top: -40px" src="uw.png"> 
	<h1 class="site-title" style="font-size: 30px;"><a href="https://sandeepsilwal.com/">Sandeep Silwal</a></h1>

	<div class="site-description"><h2></h2>
	</div>

	<nav class="nav">
		<ul class="flat">
			<li>
				<a href="https://sandeepsilwal.com">Home</a>
			</li>

			<li>
				<a href="https://sandeepsilwal.com/teaching">Teaching</a>
			</li>
			
		</ul>
	</nav>
</div>



<h1 class="page-title">Publications</h1>

		<h4> I've tried to include a brief 'story' for most papers. My goal is to highlight the context in which it was written and what I find interesting about the problem and results. </h4>

	<div class="papers">  
		<a href="javascript:story('LearningAugmented')"><h2 style="padding-bottom:10px"> > Learning-Augmented and Data-Driven Algorithm Design</h2></a>

		<div id="LearningAugmented" style="display: none;">

		<ul>

			<li>  
	  			<p> <b> Improved Approximations for Hard Graph Problems using Predictions  </b>
	  				<br> Anders Aamand, Justin Y. Chen, Siddharth Gollapudi, Sandeep Silwal, Hao Wu
	  				<br> <em> ICML 2025</em>  [<a href="https://arxiv.org/abs/2505.23967">pdf</a>] [<a href="">story coming soon!</a>] </p>

	  		</li>



			<li>  
	  			<p> <b> Learning-Augmented Frequent Directions </b>
	  				<br> Anders Aamand, Justin Y. Chen, Siddharth Gollapudi, Sandeep Silwal, Hao WU  
	  				<br> <em> <b>Spotlight Presentation</b> at ICLR 2025</em>  [<a href="https://arxiv.org/abs/2503.00937">pdf</a>] [<a href="">story coming soon!</a>] </p>

	  		</li>






			
			<li>  
	  			<p> <b>Improved Frequency Estimation Algorithms with and without Predictions </b>
	  				<br> Anders Aamand, Justin Y. Chen, Huy Nguyen, Sandeep Silwal, Ali Vakilian 
	  				<br> <em> <b>Spotlight Presentation</b> at NeurIPS 2023</em>  [<a href="https://arxiv.org/abs/2312.07535">pdf</a>] [<a href="javascript:story('ACNSV23')">story</a>] [<a href="https://https://www.youtube.com/watch?v=6nyH32X_p-s">video</a>] </p>


	  			<div id="ACNSV23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We consider the fundamental problem of estimating frequencies of items arriving in a stream, given access to an oracle which predicts "heavy-hitters". Prior work gave theoretical guarantees for this setting under the further (natural) assumption that the underlying frequencies follow a Zipfian pattern. We show that under the Zipfian assumption, a much better frequency estimation algorithm already exists without the use of any predictions. Additionally incorporating predictions further improves its performance, as shown in our experiments. </p></div>

	  		</li>

			
			<li>  
	  			<p> <b>Learning-Augmented Algorithms for Online Linear and Semidefinite Programming </b>
	  				<br> Elena Grigorescu, Young-San Lin, Sandeep Silwal, Maoyuan Song, Samson Zhou
	  				<br> <em> NeurIPS 2022 </em> [<a href="https://arxiv.org/abs/2209.10614">pdf</a>]  </p>

	  		</li>


			<li>  
	  			<p> <b>Faster Fundamental Graph Algorithms via Learned Predictions </b>
	  				<br> Justin Y. Chen, Sandeep Silwal, Ali Vakilian, Fred Zhang
	  				<br> <em> ICML 2022 </em> [<a href="https://arxiv.org/abs/2204.12055">pdf</a>] [<a href="javascript:story('CSVZ22')">story</a>] [<a href="https://slideslive.com/38984142/faster-fundamental-graph-algorithms-via-learned-predictions">video</a>] </p>

	  				<div id="CSVZ22" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We extend the work of Dinitz et al. (NeurIPS, 2021) and study graph algorithms with the help of learned LP variables. We obtain a better bound for bipartite matching by taking the Hungarian method inspired algorithm of Dinitz et al. and incorporating a a flow perspective. We also give a learned variant of the classic Goldberg's algorithm for shortest-paths with negative edges. Interestingly, the vertex potentials that are often used in shortest-path algorithms are also the variables of an appropriate LP. Lastly, we obtain a host of other learning-based results by utilizing standard (efficient) reductions between graph problems. An interesting development was that for the matching case, the challenge was to utilize a (feasible) set of dual predictions whereas for shortest-paths, the challenge was to round a given set of predictions to first be feasible. After that, optimizing is easy: just run Dijkstra's! Our experiments were also on an interesting dataset: graphs where nodes represent countries and edges are currency exchange rates. By taking the logs of the rates, finding the most efficient way to transfer one currency to another is a cool textbook example of shortest-paths in action. </p></div>

	  		</li>


			<li>  
	  			<p> <b>Triangle and Four Cycle Counting with Predictions in Graph Stream </b>
	  				<br> Justin Y. Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, Tal Wagner, David Woodruff, Michael Zhang
	  				<br> <em> ICLR 2022 </em> [<a href="https://arxiv.org/pdf/2203.09572.pdf">pdf</a>] [<a href="javascript:story('CEILNRSWWZ22')">story</a>] [<a href="https://www.dropbox.com/s/norgodz77wz22w5/Triangles%20ICLR%2022.pdf?dl=0">slides</a>] [<a href="javascript:story('CEILNRSWWZ22_code')">code</a>] </p>

	  				<div id="CEILNRSWWZ22" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> Counting triangles in a graph stream is hard: we might accidentally miss 'important' edges in the stream which contribute a lot to the global triangle count. This is where a predictor comes in: if we are given hints on which edges are important, the problem can be mitigated by simply keeping them, leading to improved space bounds. While simple, the observation is quite natural. In practice, there are graph datasets which are slow varying across time (think social network for ex). Theoretically, it turns out many prior algorithms can be reframed in the viewpoint of having access to a predictor. </p></div>

	  				<div id="CEILNRSWWZ22_code" style="display: none; border:1.5px solid black;padding:3px;"><p> See the supplementary material <a href="https://openreview.net/forum?id=8in_5gN9I0">here</a>. </p></div>
	  		</li>

	  		<li>  
	  			<p> <b>Learning-Augmented $k$-means Clustering </b> 
	  				<br> Jon Ergun, Zhili Feng, Sandeep Silwal, David Woodruff, Samson Zhou

	  				<br> <em> <b>Spotlight Presentation</b> at ICLR 2022</em> [<a href="https://arxiv.org/pdf/2110.14094.pdf">pdf</a>] [<a href="javascript:story('EFSWZ22')">story</a>] [<a href="https://www.dropbox.com/s/g069ubapd9xr5sg/k%20means%20iclr%2022.pdf?dl=0">slides</a>] </p>

					<div id="EFSWZ22" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We consider the problem of $k$-means clustering with the help of a predictor which outputs 'noisy' labels for each point. If the predictor's hints are based on a sufficiently accurate clustering, we can obtain a clustering which can be arbitrarily close to optimum, thereby surpassing known approximation lower bounds (without predictions). Interestingly, it is not sufficient to just blindly follow the predictor's advice and one must post-process the hints. Our algorithms are inspired from robust mean estimation. The fact that we can overcome worst-case NP-hardness via learning is very cool to me and is something that should be explored more. </p></div>
	  		</li>



			<li>
	  			<p> <b>Learning-based Support Estimation in Sublinear Time</b> 
	  			<br> Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, Tal Wagner

	  			<br> <em> <b>Spotlight Presentation</b> at ICLR 2021 </em> [<a href="https://arxiv.org/abs/2106.08396">pdf</a>] [<a href="javascript:story('EINRSW21')">story</a>] [<a href="javascript:story('EINRSW21_video')">video</a>] [<a href="https://www.dropbox.com/s/pcef3coyd6oh9jm/support%20poster%20iclr%2021.pdf?dl=0">poster</a>] </p>

				<div id="EINRSW21" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We consider the sample complexity of estimating the support size of a discrete distribution drawn from a subset of a possibly large domain $[n]$. Without any advice, the sample complexity is $O(n/\log n)$, barely sublinear. In our model, we assume approximate probabilities of each sample as advice. With this augmentation, the sample complexity drops significantly to $O(n^c)$ for $c &lt; 1$. Our experiments were based on internet search queries for which we could learn approximate underlying probabilities from historical data.</p></div>

				<div id="EINRSW21_video" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> <a href="https://harvard.zoom.us/rec/play/9hC2pgMhN_a-tQ3hzxJ6mBwsErS9ecjmwucPGSFjJrJ18NE4zDv5m4aP1AU9QAIOFV_GU5y4jO4a5S0.Azqt-cHOzyVNiox9?continueMode=true&_x_zm_rtaid=OvAuHrM3TySIM5t7Dvmv_w.1653660390076.17fb8b7ed468b3a9226c3a934ad0be8f&_x_zm_rhtaid=900">45 min video</a>  <br> <a href="https://iclr.cc/virtual/2021/poster/2622">10 min video</a>  </p></div>

	  		</li>


		</ul>

		</div>

		<a href="javascript:story('Similarity')"><h2 style="padding-bottom:10px"> > Fast Similarity Computation</h2></a>

		<div id="Similarity" style="display: none;">

		<ul>


		<li>  
	  			<p> <b> Even Faster Kernel Matrix Linear Algebra via Density Estimation   </b>
	  				<br>  Rikhav Shah, Sandeep Silwal, Haike Xu
	  				<br> <em> Preprint  </em>   [<a href="https://arxiv.org/abs/2510.02540">pdf</a>] [<a href="">story coming soon!</a>] </p>

	  	</li>



			<li>  
	  			<p> <b> A Bi-metric Framework for Fast Similarity Search  </b>
	  				<br> Haike Xu*, Sandeep Silwal, Piotr Indyk
	  				<br> <em> <b>Best Paper Award </b> at The 1st Workshop on Vector Databases, ICML 2025</em> 
	  				<br> <em> Preprint</em>  [<a href="https://arxiv.org/abs/2406.02891">pdf</a>] [<a href="javascript:story('XSI24')">story</a>] [<a href="https://www.youtube.com/watch?v=W1ww23ZJC3s">video</a>] </p>

		  			<div id="XSI24" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p>  We propose a new "bi-metric" framework for designing nearest neighbor data structures. Our framework assumes two dissimilarity functions: a *ground-truth* metric that is accurate but expensive to compute (e.g., embeddings from a large or expensive neural network to compare two sentences), and a *proxy* metric that is cheaper but less accurate (e.g., embeddings from a very small or local network). In both theory and practice, we show how to construct data structures using only the proxy metric such that the query procedure achieves the accuracy of the expensive metric, while only using a limited number of calls to both metrics. In many cases, we can beat the popular "re-ranking" baseline through the use of some pretty cool graph-based data-structures!</p></div>
	  		</li>


	  		<li>  
	  			<p> <b> A Near-Linear Time Algorithm for the Chamfer Distance </b>
	  				<br> Ainesh Bakshi, Piotr Indyk, Rajesh Jayaram, Sandeep Silwal, Erik Waingarten
	  				<br> <em> NeurIPS 2023</em>  [<a href="https://arxiv.org/abs/2307.03043">pdf</a>] [<a href="javascript:story('BIJSW23')">story</a>] [<a href="https://www.youtube.com/watch?v=yhrRs6P3KKk">video</a>]</p>

	  			<div id="BIJSW23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> A popular measure of similarity between two large scale high-dimensional datasets is the optimal transport (or Earth Movers) distance. Unfortunately, this is a computationally expensive measure to compute. In this paper, we consider fast algorithms for computing the Chamfer distance, a relaxation of optimal transport. It is defined as the sum of nearest neighbor distances from every point in one dataset to the other. We give a linear time algorithm for computing this value and show that interestingly, outputting the underlying mapping (as opposed to just the value) between points that defines the Chamfer distance (conditionally) requires quadratic time.</p></div>
	  		</li>





			<li>  
	  			<p> <b>KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals </b>
	  				<br> Sandeep Silwal*, Sara Ahmadian, Andrew Nystrom, Andrew McCallum, Deepak Ramachandran, Seyed Mehran Kazemi
	  				<br> <em> ICLR 2023 </em> [<a href="https://openreview.net/pdf?id=p0JSSa1AuV">pdf</a>] [<a href="javascript:story('SANMRK23')">story</a>] [<a href="https://www.dropbox.com/scl/fi/lohyojll541ovvfxccydl/Kwikbucks-ICLR-Pres.pdf?rlkey=xg60r5v33z4uiniyqoj8uw42l&st=451gfb6z&dl=0">slides</a>] </p>

	  				<div id="SANMRK23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> The work explores if we can efficiently query large ML models, more precisely large language models, to solve concrete downstream tasks. We focus on clustering: we are given a set of $n$ sentences of text documents and query access to a large LLM which provides high-quality pairwise similarity scores for every pair of sentences of texts. Ideally, we would query the model on all $\Theta(n^2)$ pairs to obtain a high-quality similarity matrix, which can be used for many types of clustering algorithms downstream. However, this is not feasible in practice due to the prohibitive resource costs required to make a large number of queries. For ex, even if $n = 10^6$, finding the 'ground truth' similarities as computed by the model requires on the order of $10^{12}$ queries! We solve this challenging problem via an algorithmic lens. First we narrow down to the concrete clustering problem of correlation clustering. Then, we use the fact that often times, very efficient but lower quality hints/predictions are available for text similarity. Using the idiosyncrasies of correlation clustering coupled with readily available hints, we are able to query the model a very limited number of times while still obtaining a clustering comparable to the hypothetical case where querying all pairs is feasible. We also theoretically model the setting and show that for correlation clustering, a linear number of queries is sufficient to obtain close to the optimal clustering quality. </p></div>
	  		</li>

			<li>  
	  			<p> <b>Sub-quadratic Algorithms for Kernel Matrices via Kernel Density Estimation </b>
	  				<br> Ainesh Bakshi, Praneeth Kacham, Piotr Indyk, Sandeep Silwal, Samson Zhou
	  				<br> <em> <b>Spotlight Presentation</b> at ICLR 2023</em>  [<a href="https://arxiv.org/abs/2212.00642">pdf</a>] [<a href="javascript:story('BKISZ23')">story</a>] [<a href="https://iclr.cc/virtual/2023/oral/12713">video and slides</a>] </p>

				<div id="BKISZ23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p>We noticed there were many works on designing awesome datastructures for kernel density estimation: they preprocess a dataset and output a datastructure which approximates the kernel density value at any future query in sublinear time. However, there were not many works on using these powerful datastructures in downstream applications. In this work, we show how to use these datastructures to perform fast sampling on kernel graphs: weighted graphs whose edges are the pairwise kernel values between datapoints. Our sampling algorithms are simple and lead to many implications for eigenvector estimation, low-rank approximation, and clustering to name a few.</p></div>

 		   </li>



 		   	<li>
	  			<p> <b>Faster Linear Algebra for Distance Matrices</b>  
	  			<br> Piotr Indyk, Sandeep Silwal

	  			<br> <b>Oral Presentation</b>  <em> at NeurIPS 2022 </em> [<a href="https://arxiv.org/abs/2210.15114">pdf</a>] [<a href="javascript:story('IS22')">story</a>] [<a href="https://www.dropbox.com/s/po80fnnbfpkzeyq/Faster%20Linear%20Algebra%20for%20Distance%20Matrices.pdf?dl=0">slides</a>] [<a href="https://www.youtube.com/watch?v=kjfxfcZGs4o">video</a>] </p>

				<div id="IS22" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> A distance matrix is defined as the matrix $A$ which records pairwise distances between $n$ data points in $\mathbb{R}^d$. They are ubiquitous in ML and data science as they capture similarity matrices and have applications in non-linear dimensionality reduction, visualization, clustering, etc. However, they require $\Omega(n^2)$ time and space for a dataset with $n$ points, making them quite difficult to compute on. We fill this gap and explore faster and scalable algorithms for distance matrices. There are three gems in the paper. (1) For the $\ell_1$ distance metric, we can compute $Ay$ for any vector $y$ in $O(nd)$ time, which is sublinear in the matrix size! Many foundational linear algebra algorithms (power method, iterative methods etc.) use matrix-vector products as the fundamental unit so $Ay$ the 'right' algorithmic primitive to consider. (2) The natural follow up question to (1) is if all metrics admit such fast matrix-vector speedups. This is <em> not </em> the case! We show that for the $\ell_{\infty}$ metric, compute $Ay$ for general $y$ requires $\Omega(n^2)$ time, assuming standard hardness assumptions. (3) Lastly, we consider the question of computing an approximate distance matrix, for example for the $\ell_2$ metric. The most natural way to do this, if one is familiar with some algorithmic tools, is to use the Johnson-Lindenstrauss (JL) lemma: simply project your points onto $O(\log n)$ dimensions and compute the distance matrix in the projected space, which requies $\Theta(n^2 \log n)$ time. Computing distances is in fact one of the most common useage of the JL lemma. We show that one can actually <em> go beyond </em> the JL lemma and comptue an approximate distance matrix in $O(n^2 \text{poly}(\log \log n))$ time! This is the result one would get if we could project onto $O(\text{poly}(\log \log n))$ dimensions using the JL lemma, but this is false as the JL lemma is tight! We use some nice metric compression results to obtian this result.</p></div>
	  		</li>



		</ul>

		</div>

		<a href="javascript:story('Sublinear')"><h2 style="padding-bottom:10px"> > Sublinear Algorithms</h2></a>

		<div id="Sublinear" style="display: none;">

		<ul>

			


			<li>  
	  			<p> <b>Learned Interpolation for Better Streaming Quantile Approximation with Worst Case Guarantees </b>
	  				<br> Nicholas Schiefer*, Justin Y. Chen, Piotr Indyk, Shyam Narayanan, Sandeep Silwal, Tal Wagner
	  				<br> <em> ACDA 2023</em>  [<a href="https://arxiv.org/abs/2304.07652">pdf</a>] [<a href="javascript:story('SCINSW23')">story</a>] </p>

	  				<div id="SCINSW23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We improve the empirical performance of streaming quantile estimation algorithms while retaining worst-case guarantees. The main (and simple) idea is to approximate the quantiles in a stream via linear functions. This is motivated by the fact that many CDFs in the real world are quite smooth, meaning a linear interpolation obtians better approximation compared to step functions, which the state of the art theoretical algorithms use. While the idea is simple, we had a challenging time inacting it in practice to ensure both practical efficiency and theoretical gurantees. We ultimately suceeded by considering an algorithm which starts off as the standard theoretical algorithms, but switches to a linear interplation at an intermediate step, obtaining 'the best of both worlds'.</p></div>
	  		</li>


			<li>
	  			<p> <b>The White-Box Adversarial Data Stream Model</b>  
	  			<br> Miklos Ajtai, Vladimir Braverman, T.S. Jayram, Sandeep Silwal, Alec Sun, David P. Woodruff, Samson Zhou

	  			<br> <em> PODS 2022 </em> [<a href="https://arxiv.org/abs/2204.09136">pdf</a>] [<a href="javascript:story('ABJSSWZ22')">story</a>] [<a href="https://www.dropbox.com/scl/fi/38p1sskcyp7c1avbecoy8/while-box-pods-22.pptx?dl=0&rlkey=4ivm7qx4bbh15djqvlfxyrzln">slides</a>] </p>

				<div id="ABJSSWZ22" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> Streaming algorithms in the presence of an adaptive adversary has been well studied recently. Most papers are in the so called 'black-box' model where the adversary can only observe outputs of the algorithm to design future inputs. Motivated by real-world adversarial attacks, we consider a "white-box" model where the adversary can now also observe the internal randomness used by the algorithm. Unsurprisingly, many tasks become impossible without storing the entire stream. However, one can still obtain non-trivial algorithms. One example is if we assume the adversary is computationally bounded, then we can use the 'shortest integer solution' (SIS) problem to design streaming algorithms: the SIS matrix is used as a sketching matrix. Even if the adversary knows this matrix, it cannot find integer vectors in the kernel which is a property we exploit. More broadly, we employ collision-resistant hash functions as well as the classic Morris counter to design white-box robust algorithms.</p></div>
	  		</li>

			<li>
	  			<p> <b>Adversarial Robustness of Streaming Algorithms through Importance Sampling</b>  
	  			<br> Vladimir Braverman, Avinatan Hassidim, Yossi Matias, Mariano Schain, Sandeep Silwal, Samson Zhou

	  			<br> <em> <b> Silver Best Paper Award</b> at Adversarial ML Workshop at ICML 2021; Final version at NeurIPS 2021 </em> 
	  			<br>[<a href="https://arxiv.org/pdf/2106.14952.pdf">pdf</a>] [<a href="javascript:story('BHMSSZ21')">story</a>] [<a href="https://nips.cc/virtual/2021/poster/28047">video</a>] [<a href="https://www.dropbox.com/s/k8v91k7mwihma6s/importance%20sampling%20neurips%2021.pdf?dl=0">slides</a>] </p>

				<div id="BHMSSZ21" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> Adversarial robustness has garnered much recent interest in both the ML and TCS communities. Streaming algorithms are a good intermediate step: they have ML applications while being amenable to theoretical study. The adversary can adaptively design future stream elements after observing the algorithm's response to past inputs. Our simple finding is that existing streaming algorithms, e.g. those based on coresets and sampling based methods, are already robust to the stated model of adversaries. This has wide applications such as in numerical linear algebra and clustering. I believe our experiments are also interesting: they highlight the vulnerability of streaming algorithms implemented in standard and widely used software libraries such as Apache Spark. </p></div>
	  		</li>


	  		<li>
	  			<p>  <b>Property Testing of LP-Type Problems </b> 
	  			<br> Rogers Epstein, Sandeep Silwal

	  			<br> <em> ICALP 2020 </em> [<a href="https://arxiv.org/abs/1911.08320">pdf</a>] [<a href="javascript:story('ES20')">story</a>] [<a href="https://www.youtube.com/watch?v=hbGJW0ZxcHQ">video</a>] </p>

				<div id="ES20" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p>  We must determine if a constrained optimization problem is feasible or `far' from feasible while having random access to few constraints of the problem. A cool example is determining if a set of high-dimensional points, which are colored red and blue, are linearly seperable, or if a constant fraction of points need to be removed for the set to be seperable. Here the constraints correspond to having random access to the points. We show that for a natural class of optimization problems, called LP-Type problems, one only needs random access to a number of constraints which is independent of the size of the problem (and only depends on the dimension). The key idea underlying our analysis is the fact that for LP-type problems, which generalize LPs, few constraints already determine feasibility (for ex. consider the 'basis' of a LP). Such ideas were already used in prior works such as Clarkson's classic randomized algorithm for LPs and we port them over to property testing.  </p></div>
	  		
	  		</li>
	  		

	  	

		</ul>

		</div>




		<a href="javascript:story('DimReduction')"><h2 style="padding-bottom:10px"> > Dimensionality Reduction</h2></a>

		<div id="DimReduction" style="display: none;">

		<ul>


			<li>  
	  			<p> <b> Dimension Reduction for Clustering: The Curious Case of Discrete Centers  </b>
	  				<br> Shaofeng Jiang, Robert Krauthgamer, Shay Sapir, Sandeep Silwal, Di Yue
	  				<br> <em> Preprint  </em>   [<a href="https://arxiv.org/abs/2509.07444">pdf</a>] [<a href="">story coming soon!</a>] </p>

	  		</li>


			<li>  
	  			<p> <b> Randomized Dimensionality Reduction for Euclidean Maximization and Diversity Measures </b>
	  				<br> Jie Gao, Rajesh Jayaram, Benedikt Kolbe, Shay Sapir, Chris Schwiegelshohn, Sandeep Silwal, Erik Waingarten
	  				<br> <em> ICML 2025  </em>   [<a href="https://arxiv.org/abs/2506.00165">pdf</a>] [<a href="">story coming soon!</a>] </p>

	  		</li>


			<li>  
	  			<p> <b> Beyond Worst-Case Dimensionality Reduction for Sparse Vectors </b>
	  				<br> Sandeep Silwal, David Woodruff, Qiuyi Zhang  
	  				<br> <em> ICLR 2025  </em>  [<a href="https://arxiv.org/abs/2502.19865">pdf</a>][<a href="">story coming soon!</a>] </p>

	  		</li>



			<li>
	  			<p> <b>Dimensionality Reduction for Wasserstein Barycenter </b> 
	  			<br> Zachary Izzo, Sandeep Silwal, Samson Zhou
	  			<br> <em>  NeurIPS 2021 </em> [<a href="https://arxiv.org/pdf/2110.08991.pdf">pdf</a>] [<a href="javascript:story('ISZ21')">story</a>] [<a href="https://nips.cc/virtual/2021/poster/26009">video</a>] </p>

				<div id="ISZ21" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We obtain dimensionality reduction bounds for the Wasserstein barycenter problem which go beyond the classical Johnson-Lindenstrauss (JL) lemma. The key idea is to realte the problem to (constrained) $k$-means clustering for which such bounds have been known, e.g., see Makarychev et al. (STOC 2019). Surprisingly, one cannot improve upon the JL lemma if we consider the related problem of optimal transport between two discrete distributions. </p></div>
	  		</li>

			
			<li>
	  			<p> <b>Randomized Dimensionality Reduction for Facility Location and Single-Linkage Clustering</b> 
	  			<br> Shyam Narayanan*, Sandeep Silwal*, Piotr Indyk, Or Zamir
	  			<br> <em> ICML 2021 </em> [<a href="https://arxiv.org/pdf/2107.01804.pdf">pdf</a>] [<a href="javascript:story('NSIZ21')">story</a>] [<a href="javascript:story('NSIZ21_video')">video</a>] </p>

				<div id="NSIZ21" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We obtain dimensionality reduction bounds for facility location clustering and the minimum spanning tree problem which go beyond the classical Johnson-Lindenstrauss (JL) lemma. For context, the work of Makarychev et al. (STOC 2019) showed that for $k$-means clustering, one can obtain a dimension bound in terms of $\log k$, rather than $\log n$ where $n$ is the number of points. In the problems we consider, there is no parameter "$k$" to parameterize the dimension bound. Instead we use the doubling dimension, which is a measure of intrinsic dimensionality of the dataset: it is at most $O(\log n)$ in the worst-case (i.e. JL bound) but can also be $O(1)$ regardless of $n$. To obtain our bounds, we exploit 'local' properties of our problems inspired by existing algorithms. The paper has a cool experiment and figure showing that doubling dimension is not just an artifact of analysis and its impact can actually be measured in practice. </p></div>


				<div id="NSIZ21_video" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> <a href="https://www.youtube.com/watch?v=H33OlvlqRTo">40 min video</a>  <br> <a href="https://icml.cc/virtual/2021/poster/9975">5 min video</a>  </p></div>

	  		</li>
	 

	  		<li>
	  			<p><b>Using Dimensionality Reduction to Optimize t-SNE</b> 
	  			<br> Rikhav Shah, Sandeep Silwal 
	  			<br> <em> OPTML Workshop at NeurIPS 2019 </em> [<a href="https://arxiv.org/abs/1912.01098">pdf</a>] [<a href="javascript:story('SS19')">story</a>] [<a href="https://www.dropbox.com/s/26e6qhklklk9hel/JL_tsne_Opt_ML_2019_poster.pdf?dl=0">poster</a>] </p>

				<div id="SS19" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> t-SNE is a popular visualization tool which maps high dimensional data into two or three dimensions. Unfortunately it is also extremely slow. We propose a somewhat counterintuitive strategy to mitigate this: doing a dimensionality reduction (such as PCA) <em>before</em> applying t-SNE (which itself is dimensionality reduction). Qualitatively this does not seem to make a difference to the t-SNE output while making t-SNE up to an order of magnitude faster.
				</p></div>
	  		</li>

		</ul>

		</div>




		<a href="javascript:story('DifferentialPrivacy')"><h2 style="padding-bottom:10px"> > Differential Privacy</h2></a>

		<div id="DifferentialPrivacy" style="display: none;">

		
		<ul>



			<li>  
	  			<p> <b>Differentially Private Gomory-Hu Trees  </b>
	  				<br> Anders Aamand, Justin Y. Chen, Mina Dalirrooyfard, Slobodan Mitrović, Yuriy Nevmyvaka, Sandeep Silwal, Yinzhan Xu
	  				<br> <em>NeurIPS 2025</em>  [<a href="https://arxiv.org/abs/2408.01798">pdf</a>] [<a href="javascript:story('ACDMNSX24')">story</a>] </p>

	  				<div id="ACDMNSX24" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> Computing various notions of graph cuts under differential privacy (DP) has been extensively studied. Our paper fills in the following gap in this literature: ignoring privacy parameters, computing a global min-cut only incurs $\tilde{O}(1)$ error for pure DP. Computing all possible cuts incurs $\tilde{O}(n^{3/2}$ error, and computing a single min-cut incurs $\tilde{O}(n)$ error. Thus, it is natural to ask what happens if we want to compute all pairs min-cut. We show that privately reporting all pair min-cuts only incurs $\tilde{O}(n)$ error, which aligns with the single minimum-cut case. Along the way, we also release a private Gomory-Hu tree, which neatly captures all minimum cuts in a tree structure. An interesting open question raised by our work is if $\tilde{O}(n)$ error can be improved on if we just want all pair min-cut values, rather than the cuts themselves. </p></div>

	  		</li>


	  		<li>  
	  			<p> <b> Breaking the $n^{1.5}$ Additive Error Barrier for Private and Efficient Graph Sparsification via Private Expander Decomposition</b>
	  				<br> Anders Aamand, Justin Y. Chen, Mina Dalirrooyfard, Slobodan Mitrović, Yuriy Nevmyvaka, Sandeep Silwal, Yinzhan Xu
	  				<br> <em> ICML 2025</em>   [<a href="https://arxiv.org/abs/2507.01873">pdf</a>][<a href="">story coming soon!</a>] </p>

	  		</li>



	  		<li>  
	  			<p> <b>Efficiently Computing Similarities to Private Datasets </b>
	  				<br> Arturs Backurs, Zinan Lin, Sepideh Mahabadi, Sandeep Silwal, Jakub Tarnawski
	  				<br> <em> ICLR 2024</em>  [<a href="https://arxiv.org/abs/2403.08917">pdf</a>] [<a href="javascript:story('BLMST23')">story</a>] [<a href="https://www.youtube.com/watch?v=WI4B9ep5is0">video</a>] </p>


	  			<div id="BLMST23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> Imagine a hospital which has a (private) dataset of patients with a particular ailment. We would like to measure the similarity between this dataset and any new patient in hopes of diagnosing them. The paper abstracts this natural problem as follows: given a datset $X \subset \mathbb{R}^d$, output a private datastructure $D$. On any query vector $y$, $D(y)$ computes $\sum_{x \in X} f(x,y)$ where $f$ is a desired similarity function (e.g. distance function or kernel function). We want $D$ itself to be private so it can handle an arbitrary number of queries (for ex, image if $D$ always outputs $42$. This is private but not very accurate so we want to do better). The paper explores several natural choices of $f$ and shows how to exploit the "geometry" of $f$ to create $D$ with low error.</p></div>

	  		</li>



			<li>
	  			<p> <b>Robust Algorithms on Adaptive Inputs from Bounded Adversaries </b>  
	  				<br> Yeshwanth Cherapanamjeri, Sandeep Silwal, David P. Woodruff, Fred Zhang, Qiuyi Zhang, Samson Zhou
	  				<br> <em> ICLR 2023 </em> [<a href="https://arxiv.org/abs/2304.07413">pdf</a>] [<a href="javascript:story('CSWZZZ23')">story</a>] [<a href="https://www.dropbox.com/scl/fi/ovfcuwd5sg5gjmwsdsfg4/iclr23_robust_poster.pdf?rlkey=fzhsa4l73c2vliaeyisjk3w3n&dl=0">poster</a>] </p>


	  				<div id="CSWZZZ23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> Inspired by related works in streaming, we use differentiall privacy to design adversarially robust algorithms in many offline settings. The cool observation used is that we can view the internal randomness of randomized algorithms as the 'private' dataset which we wish to protect from adversaries via differential privacy. </p></div>

	  		</li>




		</ul>

		</div>





		<a href="javascript:story('BroaderAlgorithmicWorks')"><h2 style="padding-bottom:10px"> > Broader Algorithmic Works</h2></a>


		<div id="BroaderAlgorithmicWorks" style="display: none;">


		<ul>


			<li>  
	  			<p> <b>Optimal and learned algorithms for the online list update problem with Zipfian accesses </b>
	  				<br> Piotr Indyk, Isabelle Quaye, Ronitt Rubinfeld, Sandeep Silwal 
	  				<br> <em> ALT 2025</em>  [<a href="https://proceedings.mlr.press/v272/indyk25a.html">pdf</a>]</p>
	  		</li>




			<li>  
	  			<p> <b>Constant Approximation for Individual Preference Stable Clustering </b>
	  				<br> Anders Aamand, Justin Y. Chen, Allen Liu, Sandeep Silwal, Pattara Sukprasert, Ali Vakilian, Fred Zhang
	  				<br> <em> <b>Spotlight Presentation</b> at NeurIPS 2023</em>  [<a href="https://arxiv.org/abs/2309.16840">pdf</a>] [<a href="https://nips.cc/virtual/2023/poster/72625">video</a>] </p>
	  		</li>



			<li>
	  			<p> <b>Optimal Algorithms for Linear Algebra in the Current Matrix Multiplication Time </b>  
	  				<br> Yeshwanth Cherapanamjeri, Sandeep Silwal, David P. Woodruff, Samson Zhou
	  				<br> <em> SODA 2023 </em> [<a href="https://arxiv.org/abs/2211.09964">pdf</a>] </p>

	  		</li>



			<li>
	  			<p> <b>Motif Cut Sparsifiers </b>  
	  				<br> Michael Kapralov, Mikhail Makarov, Sandeep Silwal, Christian Sohler, Jakab Tardos
	  				<br> <em> FOCS 2022 </em> [<a href="https://arxiv.org/abs/2204.09951">pdf</a>] [<a href="javascript:story('KMSST22')">story</a>] </p>


	  				<div id="KMSST22" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> Much work has gone into sparsifying dense graphs to preserve the size of cuts. In this work, we ask a new question: can we sparsify graphs to preserve triangle counts, or in general counts of small subgraphs, across cuts? </p></div>

	  		</li>


			<li>  
	  			<p> <b>Exponentially Improving the Complexity of Simulating the Weisfeiler-Lehman Test with Graph Neural Networks</b>
	  				<br> Anders Aamand, Justin Y. Chen, Piotr Indyk, Shyam Narayanan, Nicholas Schiefer, Ronitt Rubinfeld, Sandeep Silwal, Tal Wagner
	  				<br> <em> NeurIPS 2022 </em> [<a href="https://arxiv.org/abs/2211.03232">pdf</a>][<a href="javascript:story('ACINSRSW22')">story</a>]  [<a href="https://www.dropbox.com/s/at35jjn50y937hl/Exponentially%20Improving%20the%20Complexity%20of%20Simulating%20the%20WL%20Test%20with%20GNNS.pdf?dl=0">slides</a>] </p>

	  				<div id="ACINSRSW22" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> The Weisfeiler-Lehman (WL) test is a popular heuristic for graph isomorphism. Its claim to fame in the GNN community is the theoretical characterization of Xu et al. which states that message passing GNNs can simulate the WL test and furthermore, these GNNs can only distinguish graphs as well as the WL test. Such works provide a nice qualitative understanding of the expressibility powers of GNNs, but leave opon the question of <em> quantitative </em> understanding which our works fills. We provide an almost tight understanding of the sizes for GNNs (in terms of the number of ReLu units and message size used) to simulte the WL test. The main highlight result is that one can use an almost logarithmic (in the size of the input graph) number of ReLu units and message size to perform the simulation, which provides an exponential improvement over prior works. Along the way we construct a nice primitive for performing hashing using GNNs. </p></div>

	  		</li>


	  		<li>  
	  			<p> <b>Hardness and Algorithms for Robust and Sparse Optimization</b>
	  				<br> Eric Price, Sandeep Silwal, Samson Zhou
	  				<br> <em> ICML 2022 </em> [<a href="https://arxiv.org/abs/2206.14354">pdf</a>] [<a href="javascript:story('PSZ22')">story</a>] [<a href="https://www.dropbox.com/s/glvxzw5w8qmr9ef/__Hardness%20and%20Algorithms%20for%20Sparse%20Optimization.pdf?dl=0">slides</a>] </p>

	  				<div id="PSZ22" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> Optimization problems where one requires sparsity of variables, such as sparse regression or sparse PCA, are an important and well-studied class of problems. Many of these problems are known to be hard in the worst case, and the best theoretical algorithms have an exponential dependence on $k$, the sparsity parameter. We consider a natural version where we are promised that an optimal solution exists. Given this, we design algorithms for such problems with a reduced exponential dependence of $k/2$, all using the same underlying idea. To illustrate this, suppose we want to solve $Ax=b$ where we are promised a $k$-sparse solution $x$ exists. We first guess over all $k/2$ sparse vectors. We then "match" a fixed guess $y$ to the "correct" other $k/2$ sparse vector using fast nearest neighbor search data structures on the query $b-Ay$. This is the classical algorithmic idea of "meet in the middle." We also study (fine-grained) hardness of various formulations of sparse optimization problems and give reductions between sparse and robust regression, filling a gap in the literature.  </p></div>

	  		</li>


			<li>
	  			<p> <b>Smoothed Analysis of the Condition Number Under Low-Rank Perturbations</b>  
	  				<br> Rikhav Shah, Sandeep Silwal  
	  				<br> <em> RANDOM 2021 </em> [<a href="https://arxiv.org/pdf/2009.01986.pdf">pdf</a>] [<a href="javascript:story('SS21')">story</a>] [<a href="https://www.youtube.com/watch?v=gEE1jG4Oejs">video</a>] </p>

	  				<div id="SS21" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> Most works in smoothed analysis assume dense entry-wise perturbation of the input variables. We take a step back and consider a new model of low-rank perturbations. Our main result is that adding a random rank $k$ matrix to a fixed matrix of rank at-least $n-k$ produces a well-conditioned matrix. </p></div>

	  		</li>



		</ul>

		</div>



	<a href="javascript:story('Distributions')"><h2 style="padding-bottom:10px"> > Distribution Learning and Testing</h2></a>

		<div id="Distributions" style="display: none;">

		<ul>


			<li>  
	  			<p> <b>  Hypothesis Selection: A High Probability Conundrum s</b>
	  				<br> Anders Aamand, Maryam Aliakbarpour, Justin Y. Chen, Sandeep Silwal
	  				<br> Preprint [<a href="https://arxiv.org/abs/2509.03734">pdf</a>] [<a href="">story coming soon!</a>]  </p>

	  		</li>



			<li>  
	  			<p> <b> On the Structure of Replicable Hypothesis Testers</b>
	  				<br> Anders Aamand, Maryam Aliakbarpour, Justin Y. Chen, Shyam Narayanan, Sandeep Silwal
	  				<br> SODA 2026 [<a href="https://arxiv.org/abs/2507.02842">pdf</a>] [<a href="">story coming soon!</a>]  </p>

	  		</li>


	  		
	  		<li>  
	  			<p> <b> Statistical-Computational Tradeoffs for Density Estimation </b>
	  				<br>  Anders Aamand, Alexandr Andoni, Justin Y. Chen, Piotr Indyk, Shyam Narayanan, Sandeep Silwal, Haike Xu 
	  				<br> <em> NeurIPS 2024</em>   [<a href="https://arxiv.org/abs/2410.23087">pdf</a>][<a href="">story coming soon!</a>] </p>
	  		</li>


	  		<li>  
	  			<p> <b> Optimal Algorithms for Augmented Testing of Discrete Distributions </b>
	  				<br> Maryam Aliakbarpour, Piotr Indyk, Ronitt Rubinfeld, Sandeep Silwal 
	  				<br> <em> NeurIPS 2024</em>  [<a href="https://arxiv.org/abs/2412.00974">pdf</a>] [<a href="">story coming soon!</a>] </p>

	  		</li>


	  		<li>  
	  			<p> <b>Data Structures for Density Estimation </b>
	  				<br> Anders Aamand, Alexandr Andoni, Justin Y Chen, Piotr Indyk, Shyam Narayanan, Sandeep Silwal
	  				<br> <em> ICML 2023</em>  [<a href="https://arxiv.org/abs/2306.11312">pdf</a>]  [<a href="javascript:story('AACINS23')">story</a>] [<a href="https://www.dropbox.com/scl/fi/kzc3gjt790786y8ouyrob/density_icml.pdf?rlkey=v0w3ml8xkug70d58xnpdyp958&dl=0">slides</a>] [<a href="https://slideslive.com/39003057/data-structures-for-density-estimation">video</a>]</p>

				<div id="AACINS23" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We consider the following problem: we are given knowledge of $k$ discrete distributions $v_i$ for $1 \le i \le k$ over the domain $[n] = \{1, \cdots ,n\}$ which we can preprocess. Then we get samples from an unknown discrete distribution $p$, also over $[n]$. The goal is to output the closest distribution to $p$ among the $v_i$’s in TV distance (up to some small additive error). The state of the art algorithms require $\Theta(\log k)$ samples and run in near linear time.

				We introduce a fresh perspective on the problem and ask if we can output the closest distribution in sublinear time. This question is particularly motivated as the problem is a generalization of the traditional nearest neighbor search problem: if we take enough samples, we can learn $p$ explicitly up to low TV distance and then find the closest $v_i$ in $o(k)$ time using standard nearest neighbor search algorithms. However, this approach requires $\Omega(n)$ samples. Thus, it is natural to ask: can we obtain both sublinear number of samples and sublinear query time? We present some nice progress towards this question. </p></div>

	  		</li>


			<li>
	  			<p> <b>Testing Properties of Multiple Distributions with Few Samples </b>  
	  			<br> Maryam Aliakbarpour, Sandeep Silwal 

	  			<br> <em> ITCS 2020 </em> [<a href="https://arxiv.org/abs/1911.07324">pdf</a>] [<a href="javascript:story('AS20')">story</a>] [<a href="javascript:story('AS20_video')">video</a>]  </p>

				<div id="AS20" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> We consider a noisy version of distribution testing where each sample can be drawn from a possibly different distribution. While this is hopeless in general, we study natural models of similarity across distributions where one can obtain meaningful results. </p></div>

				<div id="AS20_video" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> See the first 20 mins. <a href="https://www.youtube.com/watch?v=8wmq9zDiJN4">here</a>  </p></div>
	  		</li>



		</ul>

	    </div>






		<!-- <a href="javascript:story('Miscellaneous')"><h2 style="padding-bottom:10px"> > Miscellaneous</h2></a>

		<div id="Miscellaneous" style="display: none;">

		<ul>


			<li>
	  			<p> <b>Cluster Tree for Nearest Neighbor Search </b>  
	  				<br> Dan Kushnir, Sandeep Silwal 
	  				<br> <em> TMLR </em> [<a href="https://openreview.net/forum?id=ELtNtkGXoK">pdf</a>] </p>

	  		</li>


			<li>
	  			<p> <b>Improved Space Bounds for Learning with Experts </b>  
	  				<br> Anders Aamand, Justin Y. Chen, Huy Lê Nguyễn, Sandeep Silwal
	  				<br> <em> ACDA 2023 (Poster) </em> [<a href="https://arxiv.org/abs/2303.01453">pdf</a>] </p>

	  		</li>


	  		<li>  
	  			<p> <b>A Concentration Inequality for the Facility Location Problem </b> 
	  				<br> Sandeep Silwal
	  				<br> <em> Operations Research Letters, Volume 50 </em> [<a href="https://arxiv.org/pdf/2012.04488.pdf">pdf</a>] [<a href="javascript:story('SORL')">story</a>] </p>

	  				<div id="SORL" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> How well does the objective value of a random instance of the (uniform) facility location problem concentrate? The answer is quite surprising. In the unit square with $n$ uniform points, the interval of concentration is at most $O(n^{1/6})$, an odd exponent. The argument requires Talagrand's concentration inequality which heavily exploits the 'local' structure of the facility location problem. </p></div>
	  		</li>



	  		<li>
	  			<p> <b> A note on the universality of ESDs of inhomogeneous random matrices </b> 
	  				<br> Vishesh Jain, Sandeep Silwal
	  				<br> <em>  Latin American Journal of Probability and Mathematical Statistics </em> [<a href="https://arxiv.org/pdf/2006.05418.pdf">pdf</a>] </p>
	  		</li>


	  		<li>  
	  			<p> <b>Directed Random Geometric Graphs </b> 
	  				<br> Jesse Michel, Ramis Movassagh, Sushruth Reddy, Rikhav Shah, Sandeep Silwal
	  				<br> <em> Journal of Complex Networks </em> [<a href="https://arxiv.org/abs/1808.02046">pdf</a>] [<a href="javascript:story('MMRSS')">story</a>] </p>

	  				<div id="MMRSS" style="display: none; border:1.5px solid black;padding:3px;margin-bottom:3px;"><p> A novel model of random digraphs for real-world directed networks inspired by the classical random geometric graph model. Interestingly, the in-degree and out-degree distributions follow different laws (power-law vs binomial). </p></div>
	  		</li>


		</ul>

	    </div>
 -->

	<h5>*Authors are always listed in alphabetical order, except if authors are starred, in which case authors are listed in contribution order and starred authors have (equal) first-author contribution.</h5>

		
	</div>
</div>

	<div class="footer wrapper">
	<nav class="nav">
		<div> <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a> </div>
	</nav>
</div>


</body>

</html>
